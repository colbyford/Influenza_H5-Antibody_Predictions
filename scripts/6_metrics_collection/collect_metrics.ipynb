{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Metrics from HADDOCK 3 Experiment Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in Experiments worksheet\n",
    "experiments = pd.read_excel('../../Experiments.xlsx', sheet_name='Experiments')#.head(0)\n",
    "## For testing...\n",
    "# experiments = experiments.append({'experiment_id': 'TEST_5A3I'}, ignore_index=True)\n",
    "\n",
    "# experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create empty dataframe to store results\n",
    "metrics_df = pd.DataFrame(columns=['experiment_id', 'cluster_rank', 'cluster_id', 'n', 'under_eval',\n",
    "                                   'score', 'score_std', 'irmsd', 'irmsd_std',\n",
    "                                   'fnat', 'fnat_std', 'lrmsd', 'lrmsd_std', 'dockq', 'dockq_std',\n",
    "                                   'air', 'air_std', 'bsa', 'bsa_std',\n",
    "                                   'desolv', 'desolv_std', 'elec', 'elec_std',\n",
    "                                   'total', 'total_std', 'vdw', 'vdw_std',\n",
    "                                   'caprieval_rank', 'best_pdb_path'])\n",
    "\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create empty dataframe to store results\n",
    "ss_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics for experiment: 12H5__EPI1158808\n",
      "Getting metrics for experiment: FLD21.140__EPI355443\n"
     ]
    }
   ],
   "source": [
    "for index, experiment in experiments.iterrows():\n",
    "    experiment_id = experiment['experiment_id']\n",
    "\n",
    "    ## Check if the output/ directory exists\n",
    "    outputs_dir = f\"../../data/experiments/{experiment_id}/output/10_caprieval\"\n",
    "    if os.path.exists(outputs_dir):\n",
    "        print(f\"Getting metrics for experiment: {experiment_id}\")\n",
    "\n",
    "        ## Read in the individual metrics file\n",
    "        ss_metrics_path = f\"{outputs_dir}/capri_ss.tsv\"\n",
    "        ss_df_iter = pd.read_csv(ss_metrics_path, sep='\\t', comment='#')\n",
    "        ss_df_iter['experiment_id'] = experiment_id\n",
    "\n",
    "        ## If row has the lowest vdw value, flag 'best_pdb_flag' as True (otherwise False)\n",
    "        ss_df_iter['best_pdb_flag'] = ss_df_iter['vdw'].rank(method='min', ascending=True)\n",
    "        ss_df_iter['best_pdb_flag'] = ss_df_iter['best_pdb_flag'].apply(lambda x: True if x == 1 else False)\n",
    "        \n",
    "        ss_df = ss_df.append(ss_df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'md5', 'caprieval_rank', 'score', 'irmsd', 'fnat', 'lrmsd',\n",
       "       'ilrmsd', 'dockq', 'cluster-id', 'cluster-ranking',\n",
       "       'model-cluster-ranking', 'air', 'angles', 'bonds', 'bsa', 'cdih',\n",
       "       'coup', 'dani', 'desolv', 'dihe', 'elec', 'improper', 'rdcs', 'rg',\n",
       "       'total', 'vdw', 'vean', 'xpcs', 'experiment_id', 'best_pdb_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_df.columns.values\n",
    "\n",
    "## Columns to discard\n",
    "discard_cols = ['md5', 'cluster-id', 'cluster-ranking', 'model-cluster-ranking',\n",
    "                'angles', 'bonds', 'cdih', 'coup', 'dani', 'improper',\n",
    "                'rdcs', 'rg', 'vean', 'xpcs']\n",
    "\n",
    "## Columns to keep\n",
    "meta_cols = ['experiment_id', 'model', 'best_pdb_flag', 'caprieval_rank']\n",
    "\n",
    "metric_cols = ['score', 'irmsd', 'fnat', 'lrmsd', 'ilrmsd',\n",
    "              'dockq', 'air', 'bsa', 'desolv', 'dihe',\n",
    "              'elec', 'total', 'vdw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the metrics from the best PDB\n",
    "ss_best = ss_df[ss_df['best_pdb_flag'] == True].drop(columns=discard_cols)\n",
    "\n",
    "## add \"_best\" to the column names except for 'experiment_id' and model\n",
    "ss_best.columns = [f\"{col}_best\" if col in metric_cols else col for col in ss_best.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summarize the results by experiment_id, summarizing the metrics\n",
    "ss_summary = ss_df.groupby(['experiment_id']).agg({'score': ['min','mean', 'std'],\n",
    "                                                   'irmsd': ['min', 'mean', 'std'],\n",
    "                                                   'fnat': ['min','mean', 'std'],\n",
    "                                                   'lrmsd': ['min','mean', 'std'],\n",
    "                                                   'dockq': ['max','mean', 'std'],\n",
    "                                                   'air': ['min','mean', 'std'],\n",
    "                                                   'bsa': ['max','mean', 'std'],\n",
    "                                                   'desolv': ['min','mean', 'std'],\n",
    "                                                   'elec': ['min','mean', 'std'],\n",
    "                                                   'total': ['min','mean', 'std'],\n",
    "                                                   'vdw': ['min','mean', 'std']}).reset_index()\n",
    "\n",
    "## Flatten the column names\n",
    "ss_summary.columns = ['experiment_id'] + ['_'.join(col).strip() for col in ss_summary.columns.values if col[0] in metric_cols]\n",
    "\n",
    "## Join the summary and best metrics\n",
    "ss_summary = ss_summary.merge(ss_best, on=['experiment_id'], how='left')\n",
    "\n",
    "## Reorder columns starting with the meta columns and then alphabetically thereafter\n",
    "ss_summary = ss_summary[meta_cols + sorted([col for col in ss_summary.columns if col not in meta_cols])]\n",
    "\n",
    "## Remove the 'best_pdb_flag' column\n",
    "ss_summary = ss_summary.drop(columns=['best_pdb_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>model</th>\n",
       "      <th>caprieval_rank</th>\n",
       "      <th>air_best</th>\n",
       "      <th>air_mean</th>\n",
       "      <th>air_min</th>\n",
       "      <th>air_std</th>\n",
       "      <th>bsa_best</th>\n",
       "      <th>bsa_max</th>\n",
       "      <th>bsa_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_std</th>\n",
       "      <th>total_best</th>\n",
       "      <th>total_mean</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_std</th>\n",
       "      <th>vdw_best</th>\n",
       "      <th>vdw_mean</th>\n",
       "      <th>vdw_min</th>\n",
       "      <th>vdw_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12H5__EPI1158808</td>\n",
       "      <td>../08_mdscoring/mdscoring_5.pdb</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2320.38</td>\n",
       "      <td>2320.38</td>\n",
       "      <td>2129.082500</td>\n",
       "      <td>...</td>\n",
       "      <td>-163.029</td>\n",
       "      <td>16.511306</td>\n",
       "      <td>-426.065</td>\n",
       "      <td>-386.461875</td>\n",
       "      <td>-460.116</td>\n",
       "      <td>50.024223</td>\n",
       "      <td>-88.917</td>\n",
       "      <td>-72.713250</td>\n",
       "      <td>-88.917</td>\n",
       "      <td>11.899694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLD21.140__EPI355443</td>\n",
       "      <td>../08_mdscoring/mdscoring_10.pdb</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2249.86</td>\n",
       "      <td>2249.86</td>\n",
       "      <td>2112.828889</td>\n",
       "      <td>...</td>\n",
       "      <td>-138.378</td>\n",
       "      <td>15.918808</td>\n",
       "      <td>-416.168</td>\n",
       "      <td>-356.645556</td>\n",
       "      <td>-416.168</td>\n",
       "      <td>37.354104</td>\n",
       "      <td>-75.733</td>\n",
       "      <td>-64.392556</td>\n",
       "      <td>-75.733</td>\n",
       "      <td>7.767551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          experiment_id                             model  caprieval_rank  \\\n",
       "0      12H5__EPI1158808   ../08_mdscoring/mdscoring_5.pdb               1   \n",
       "1  FLD21.140__EPI355443  ../08_mdscoring/mdscoring_10.pdb               1   \n",
       "\n",
       "   air_best  air_mean  air_min  air_std  bsa_best  bsa_max     bsa_mean  ...  \\\n",
       "0       0.0       0.0      0.0      0.0   2320.38  2320.38  2129.082500  ...   \n",
       "1       0.0       0.0      0.0      0.0   2249.86  2249.86  2112.828889  ...   \n",
       "\n",
       "   score_min  score_std  total_best  total_mean  total_min  total_std  \\\n",
       "0   -163.029  16.511306    -426.065 -386.461875   -460.116  50.024223   \n",
       "1   -138.378  15.918808    -416.168 -356.645556   -416.168  37.354104   \n",
       "\n",
       "   vdw_best   vdw_mean  vdw_min    vdw_std  \n",
       "0   -88.917 -72.713250  -88.917  11.899694  \n",
       "1   -75.733 -64.392556  -75.733   7.767551  \n",
       "\n",
       "[2 rows x 49 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out to CSV\n",
    "ss_summary.to_csv('caprieval_ss_summary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
